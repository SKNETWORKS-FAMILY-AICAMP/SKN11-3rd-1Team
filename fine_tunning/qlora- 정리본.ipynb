{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b112875",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install typing_extensions==4.7.1 --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d9ab2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install peft datasets transformers bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84924c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install --upgrade pip\n",
    "# 내부적으로 사용하는 툴의 오류를 해결하기 위해"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2fa34f-f6cb-4feb-ab8c-63bec48503ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea08439-e12d-4602-8420-477c57a4f5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bdba0a-20d9-4cef-a23a-e1b213b3d1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64de6025-98b8-4715-8f6a-cfa12c573e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ffb004-69b4-4fae-9336-bc1198654095",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade transformers peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa22dac-29e9-41c8-8cee-fa9949e897cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade torch transformers peft\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ba0b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall numpy -y\n",
    "!pip install \"numpy<2\"\n",
    "!pip install faiss-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7812efa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# 허깅 페이스 로그인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418296fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
    "# AutroModelForCausalLM: GPT 기반의 생성형 모델 로드하는 클래스 / DataCollatorForSeq2Seq : 데이터 배치, 데이터 로더와 함꼐 사용, 함수 제공공\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "#PeftModel: 모델 로드드\n",
    "import bitsandbytes as bnb\n",
    "# bnb: 양자화에 도움을 줌/8bit의 연산을 지원해주는 라이브러리/모델을 좀 더 최적화하는데 도움\n",
    "import torch.nn.functional as F\n",
    "# 신경망에 적용하는 함수들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77eb92c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47b027a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "import json\n",
    "\n",
    "# 파일 경로\n",
    "file_paths = {\n",
    "    \"term\": \"./metadata/term.json\",\n",
    "    \"law\": \"./metadata/load_traffic_law.json\",\n",
    "    \"modifier\": \"./metadata/modiflier.json\",\n",
    "    \"car_case\": \"./metadata/car_to_car.json\",\n",
    "    \"law_meta\": \"./metadata/precedent.json\"\n",
    "}\n",
    "\n",
    "# 교통사고 케이스용 필드 상수\n",
    "CASE_ID = \"사건 ID\"\n",
    "CASE_TITLE = \"사건 제목\"\n",
    "CASE_SITUATION = \"사고상황\"\n",
    "BASE_RATIO = \"기본 과실비율\"\n",
    "MODIFIERS = \"케이스별 과실비율 조정예시\"\n",
    "LAW_REFERENCES = \"관련 법규\"\n",
    "LEGAL_NOTES = \"참고 판례\"\n",
    "REASON = \"기본 과실비율 해설\"\n",
    "\n",
    "# JSON 로드 함수\n",
    "def load_json(path):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# 리스트형 JSON 변환 (term, modifier, law_meta)\n",
    "def convert_list_to_documents(data_list, doc_type):\n",
    "    return [\n",
    "        Document(page_content=json.dumps(item, ensure_ascii=False), metadata={\"type\": doc_type})\n",
    "        for item in data_list\n",
    "    ]\n",
    "\n",
    "def convert_car_case_documents(data_list):\n",
    "    documents = []\n",
    "\n",
    "    def safe_value(value):\n",
    "        if isinstance(value, list):\n",
    "            return \", \".join(map(str, value))\n",
    "        elif isinstance(value, dict):\n",
    "            return json.dumps(value, ensure_ascii=False)\n",
    "        elif value is None:\n",
    "            return \"\"  # null도 허용 안 되므로 빈 문자열로 처리\n",
    "        else:\n",
    "            return str(value)\n",
    "\n",
    "    for item in data_list:\n",
    "        if not isinstance(item, dict):\n",
    "            continue\n",
    "\n",
    "        # page_content는 원본 전체 JSON 문자열\n",
    "        content = json.dumps(item, ensure_ascii=False)\n",
    "\n",
    "        # 기본 과실비율 해설이 리스트일 수 있음 → 문자열로 병합\n",
    "        reason = item.get(REASON)\n",
    "        if isinstance(reason, list):\n",
    "            reason = \"\\n\".join(map(str, reason))\n",
    "\n",
    "        metadata = {\n",
    "            \"type\": \"car_case\",\n",
    "            \"id\": safe_value(item.get(CASE_ID)),\n",
    "            \"title\": safe_value(item.get(CASE_TITLE)),\n",
    "            \"situation\": safe_value(item.get(CASE_SITUATION)),\n",
    "            \"base_ratio\": safe_value(item.get(BASE_RATIO)),\n",
    "            \"modifiers\": safe_value(item.get(MODIFIERS)),\n",
    "            \"law\": safe_value(item.get(LAW_REFERENCES)),\n",
    "            \"legal_notes\": safe_value(item.get(LEGAL_NOTES)),\n",
    "            \"reason\": safe_value(reason)\n",
    "        }\n",
    "\n",
    "        documents.append(Document(page_content=content, metadata=metadata))\n",
    "    return documents\n",
    "\n",
    "# 도로교통법 law JSON → 문서화\n",
    "def convert_law_json_to_documents(data_dict):\n",
    "    documents = []\n",
    "\n",
    "    def normalize(item):\n",
    "        return json.dumps(item, ensure_ascii=False) if isinstance(item, dict) else str(item)\n",
    "\n",
    "    for law_name, content in data_dict.items():\n",
    "        if isinstance(content, dict):\n",
    "            for clause, text in content.items():\n",
    "                lines = [normalize(x) for x in (text if isinstance(text, list) else [text])]\n",
    "                full_text = f\"{law_name} {clause}\\n\" + \"\\n\".join(lines)\n",
    "                documents.append(Document(page_content=full_text, metadata={\"type\": \"law\"}))\n",
    "        else:\n",
    "            lines = [normalize(x) for x in (content if isinstance(content, list) else [content])]\n",
    "            full_text = f\"{law_name}\\n\" + \"\\n\".join(lines)\n",
    "            documents.append(Document(page_content=full_text, metadata={\"type\": \"law\"}))\n",
    "\n",
    "    return documents\n",
    "\n",
    "\n",
    "# 문서화 실행\n",
    "term_docs       = convert_list_to_documents(load_json(file_paths[\"term\"]), \"term\")\n",
    "modifier_docs   = convert_list_to_documents(load_json(file_paths[\"modifier\"]), \"modifier\")\n",
    "law_meta_docs   = convert_list_to_documents(load_json(file_paths[\"law_meta\"]), \"law_metadata\")\n",
    "car_case_docs   = convert_car_case_documents(load_json(file_paths[\"car_case\"]))\n",
    "law_docs        = convert_law_json_to_documents(load_json(file_paths[\"law\"]))\n",
    "\n",
    "\n",
    "# 전체 문서 리스트\n",
    "all_docs = term_docs + modifier_docs + car_case_docs + law_meta_docs + law_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f56716-4e64-47ba-9306-7aeac81fec9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypeIs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1fc6fa",
   "metadata": {},
   "source": [
    "##### 모델 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794f8e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'saltlux/Ko-Llama3-Luxia-8B'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd93d078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4-bit 양자화된 모델 로드를 위한 설정\n",
    "bnb_config={\n",
    "    'load_in_4bit':True,                        # 4비트 양자화 적용할 것인지\n",
    "    'bnb_4bit_compute_dtype':torch.float16,     # 4비트의 연산을 수행할 때 어떤 데이터 타입을 쓸 것인지-torch.float16: 속도 최적화\n",
    "    'bnb_4bit_quant_type':'nf4',                # 양자화 방식에 대한 type: nf4: 4-bit의 normal float(교안 참고): 성능 개선을 위해: 정규화화\n",
    "    'device_map':'auto'                         # GPU가 여러 대일때 모두 사용할 수 있게끔 자동 설정\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a61d9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토크나이저 및 모델 로드 (모델 로드 시 4-bit 양자화 설정)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name, **bnb_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee76e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.schema import Document\n",
    "import json\n",
    "\n",
    "# 교통사고 케이스용 필드 상수\n",
    "PRECEDENT = \"참고 판례\"\n",
    "\n",
    "# 데이터 전처리 함수\n",
    "def preprocess_data(examples):  # input: dictionary\n",
    "\n",
    "    # input 데이터의 질문과 문맥\n",
    "    inputs = [court + caseid for court, caseid in zip(examples['court'], examples['case_id'])]\n",
    "\n",
    "    # 정답 데이터 추출\n",
    "    answer_texts = [content for content in examples['content']]\n",
    "\n",
    "    # 토큰화\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=4096,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        answer_texts,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=4096,\n",
    "        return_tensors='pt'\n",
    "    )['input_ids']  # 입력해주는 토큰의 id -> 정답 데이터\n",
    "\n",
    "    # # input_ids 기분으로 Labels 길이 맞춤\n",
    "    # max_length = model_inputs['input_ids'].shape[1]\n",
    "    # labels = labels[:, :max_length] # 라벨의 길이 맞춰줌\n",
    "\n",
    "    # input_ids 기분으로 Labels 길이 맞춤\n",
    "    max_length = model_inputs['input_ids'].shape[1]\n",
    "    labels = labels[:, :max_length] # 라벨의 길이 맞춰줌\n",
    "    \n",
    "    # 패딩된 부분을 -100으로 설정 : loss 계산에서 무시될 수 있게끔\n",
    "    labels[labels == tokenizer.pad_token_id] = -100\n",
    "\n",
    "    model_inputs['label'] = labels\n",
    "\n",
    "    return model_inputs\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=\"./metadata/precedent.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a19834b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affc9818",
   "metadata": {},
   "outputs": [],
   "source": [
    "precedent_doc = dataset['train'].map(preprocess_data, batched=True, remove_columns=dataset['train'].column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb198aa1",
   "metadata": {},
   "source": [
    "##### 파인 튜닝을 위한 LoRA 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfc694e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파인튜닝을 위한 LoRA 설정\n",
    "lora_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    lora_dropout=0.1,\n",
    "    bias='none',# 가중치 행렬만, 편향은 없음\n",
    "    task_type='CAUSAL_LM'# lora가 적용될 대상 모델 타입과 맞추어준다.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c32e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LoRA 적용\n",
    "\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "model.enable_input_require_grads()          # 모델에 대해 입력값으로 받은 값에 대해 gradient를 사용하게끔하는 설정\n",
    "model.gradient_checkpointing_enable()       # 체크포인트로 중간 저장: 모델이 커지게 되면 중간에 수행해준 내용들을 저장하는것이 효율적\n",
    "model.print_trainable_parameters()          # 학습가능한 파라미터의 수를 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd1c446",
   "metadata": {},
   "source": [
    "##### Training 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051d8f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 설정\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)  # 동적 데이터를 저장하면서 자동으로 다음 데이터를 불러옴: model의 타입에 맞추어 tokenizer을 사용함/\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = './q_lora_korqa',      # \n",
    "    eval_strategy='no',              # 검증: 하지 않음\n",
    "    save_strategy='steps',              #\n",
    "    per_device_train_batch_size=4,      # 배치 사이즈: 트레인\n",
    "    per_device_eval_batch_size=4,       # 배치 사이즈: eval\n",
    "    gradient_accumulation_steps=8,      # 가중치를 누산하여 한번에 계산 : 8번만큼의 가중치를 누적해 놓았다가 한번에 계산\n",
    "    learning_rate=2e-4,                 # 학습률\n",
    "    weight_decay=0.01,                  # L2 정규화 적용 비율\n",
    "    num_train_epochs=3,                 # 몇번의 epoch을 진행할 것인지\n",
    "    logging_dir='./logs',               # log 남길 dir\n",
    "    logging_steps=100,                  # log 남길 빈도수\n",
    "    save_total_limit=2,                 # 체크포인트 최대 개수: 가장 최근의 2개 저장\n",
    "    fp16=True,                          # \n",
    "    push_to_hub=False,                  # hub: hugginface에서의 허브 - true: 자동으로 hub에 저장\n",
    "    report_to='none'                    # 학습결과를 표현할 수 있는 툴에 전달달\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d961fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=precedent_doc,\n",
    "    # eval_dataset=all_docs,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa78f74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce19b9f9-8c5b-4623-a12a-a9a5a9bccde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장\n",
    "trainer.save_model(\"workspace/q_lora_korqa/checkpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16b4a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "trained_model_path='./q_lora_korqa/checkpoint-5661'\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_name) # 원본 모델에 대한 설정 로드\n",
    "config.save_pretrained(trained_model_path)      # 체크포인트 경로에 설정 저장: 재사용하기 위해해"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a610d292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습시킨 adaptor 이어붙이기\n",
    "adapter_model_path = './q_lora_korqa/checkpoint-15'\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype='auto', device_map='auto')\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model, adapter_model_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ce1027-d8db-4d9a-a666-d7d5066445e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_docs = []\n",
    "with open('all_docs', 'r') as f:\n",
    "    for line in f:\n",
    "        all_docs.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174cd704-ba79-41ab-9d55-d1dfaefbbc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY= # openAPI 넣기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d58aef",
   "metadata": {},
   "source": [
    "##### API 제한 -> 순서대로 넣기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749b3dce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma  # persist 지원\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 1. 청크 크기 조정 (500~1000 권장)\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "# 2. 문서 분할\n",
    "all_splits = text_splitter.split_documents(law_meta_docs)\n",
    "\n",
    "embedding_model = OpenAIEmbeddings(model='text-embedding-3-large', openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "# 4. Chroma DB에 배치 처리로 저장\n",
    "batch_size = 100  # 한 번에 처리할 청크 수\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=all_splits[:batch_size],  # 첫 배치\n",
    "    embedding=embedding_model,\n",
    "    persist_directory=\"./vectordb1\"\n",
    ")\n",
    "\n",
    "# 남은 청크를 순차적으로 추가\n",
    "for i in range(batch_size, len(all_splits), batch_size):\n",
    "    batch = all_splits[i:i+batch_size]\n",
    "    vectorstore.add_documents(\n",
    "        documents=batch,\n",
    "        embedding=embedding_model\n",
    "    )\n",
    "\n",
    "vectorstore.persist()  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc65408b-0057-4459-b7d5-0ca4248ff4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma # persist 지원\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "\n",
    "embedding_model = OpenAIEmbeddings(model='text-embedding-3-large', openai_api_key=OPENAI_API_KEY)\n",
    "# embedding_model = HuggingFaceEmbeddings(model_name=\"upskyy/bge-m3-korean\", encode_kwargs={'normalize_embeddings': True})\n",
    "\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=law_meta_docs,\n",
    "    embedding=embedding_model,\n",
    "    persist_directory=\"./vectordb3\",\n",
    "    collection_metadata={\"hnsw:space\": \"cosine\"}\n",
    ")\n",
    "vectorstore.persist()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5332cf6e-4d14-4e6f-a841-96caa19edbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade typing_extensions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c3c7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI\n",
    "from transformers import pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "\n",
    "# 2. 프롬프트 템플릿\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "아래 문서 내용을 바탕으로 사용자가 물어본 용어나 법률 조항, 판례에 대해 정확하고 간결하게 설명해 주세요.\n",
    "\n",
    "질문: {question}\n",
    "\n",
    "문서: {context}\n",
    "\n",
    "답변 형식:\n",
    "- 용어/조항 정의: [정확한 설명]\n",
    "- 출처가 명시된 경우: 관련 법률/조문 번호/판례명을 반드시 포함\n",
    "\n",
    "답변:\n",
    "\"\"\",\n",
    "    input_variables=[\"question\", \"context\"]\n",
    ")\n",
    "\n",
    "# 3. 리트리버 설정\n",
    "retriever_chroma = vectorstore.as_retriever(\n",
    "    search_kwargs={\n",
    "        \"k\": 5\n",
    "        }\n",
    ")\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "# 1.4 LangChain 호환 래퍼 적용\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65490bcc-773e-4afa-9076-99935d9009cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. QA 체인 구성\n",
    "qa_chain_chroma = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever_chroma,\n",
    "    chain_type=\"stuff\",\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")\n",
    "\n",
    "# 3. 직접 generate() 호출\n",
    "def generate_text(querry, max_length=512):\n",
    "    inputs = tokenizer(querry, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=max_length,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9\n",
    "    )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# 실행 예시\n",
    "print(generate_text(\"서울지방법원91가합8733\"))\n",
    "# # 5. 실제 질의 실행\n",
    "# query = \"서울지방법원91가합8733에 대해 알려줘\"\n",
    "# res_chroma = qa_chain_chroma.invoke({\"query\": query})\n",
    "\n",
    "# # 6. 출력\n",
    "# print(\"✅ 필터 적용 후 >> Chroma 답변:\\n\", res_chroma[\"result\"])\n",
    "\n",
    "\n",
    "\n",
    "# res = qa_chain.invoke({\"query\": query})\n",
    "\n",
    "# # 6. 출력\n",
    "# print(\"✅ 답변:\\n\", res[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221546dc-940b-48ad-a4da-003027c945f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. QA 체인 구성\n",
    "qa_chain_chroma = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever_chroma,\n",
    "    chain_type=\"stuff\",\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")\n",
    "\n",
    "# 3. 직접 generate() 호출\n",
    "def generate_text(querry, max_length=512):\n",
    "    inputs = tokenizer(querry, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=max_length,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9\n",
    "    )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# 실행 예시\n",
    "print(generate_text(\"서울지방법원 91가합8733에 대해 알려줘\"))\n",
    "# # 5. 실제 질의 실행\n",
    "# query = \"서울지방법원91가합8733에 대해 알려줘\"\n",
    "# res_chroma = qa_chain_chroma.invoke({\"query\": query})\n",
    "\n",
    "# # 6. 출력\n",
    "# print(\"✅ 필터 적용 후 >> Chroma 답변:\\n\", res_chroma[\"result\"])\n",
    "\n",
    "\n",
    "\n",
    "# res = qa_chain.invoke({\"query\": query})\n",
    "\n",
    "# # 6. 출력\n",
    "# print(\"✅ 답변:\\n\", res[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fad448",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b65fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "다음 '사고 상황 설명'에 대해 '문서'의 내용만 참고하여 과실 비율 및 법적 판단을 생성해 주세요.\n",
    "\n",
    "사고 상황: {question}\n",
    "\n",
    "문서 내용: {context}\n",
    "\n",
    "답변 형식:\n",
    "1. 과실비율: A차량 xx% vs B차량 xx%\n",
    "2. 사고 원인 및 판단 근거:\n",
    "   - [사고의 전개, 각 차량의 행위, 과실 요소 등을 구체적으로 설명]\n",
    "3. 관련 법률 조항:\n",
    "   - [예: 도로교통법 제10조 제2항, 제27조 제1항 등]\n",
    "4. 참고 판례:\n",
    "   - [예: 대법원 2023다12345, 서울중앙지법 2022가단123456 등]\n",
    "\n",
    "조건:\n",
    "- 반드시 문서 내용만 참고해 판단하세요.\n",
    "- 법률 조항과 판례가 명시되어 있지 않으면 유사하거나 추정 가능한 근거를 제시해도 됩니다.\n",
    "- 전체 답변은 포맷에 맞게 간결하고 전문적으로 작성하세요.\n",
    "\"\"\",\n",
    "    input_variables=[\"question\", \"context\"]\n",
    ")\n",
    "\n",
    "# 3. 리트리버 설정 : vector store 설정\n",
    "retriever_chroma = chroma_vector_store.as_retriever(\n",
    "    search_kwargs={\n",
    "        \"k\": 5,\n",
    "        \"filter\": {\n",
    "            \"case_id\": \"2009가단22343\"\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "# 4. QA 체인 구성\n",
    "qa_chain_chroma = RetrievalQA.from_chain_type(\n",
    "    llm=model,\n",
    "    retriever=retriever_chroma,\n",
    "    chain_type=\"stuff\",\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")\n",
    "\n",
    "\n",
    "# 5. 실제 질의 실행\n",
    "query = \"서울지방법원91가합8733에 대해 알려줘\"\n",
    "res_chroma = qa_chain_chroma.invoke({\"query\": query})\n",
    "\n",
    "# 6. 출력\n",
    "print(\"✅ 필터 적용 후 >> Chroma 답변:\\n\", res_chroma[\"result\"])\n",
    "\n",
    "\n",
    "# 사용 chain 넣기\n",
    "res = qa_chain.invoke({\"query\": query})\n",
    "\n",
    "# 6. 출력\n",
    "print(\"✅ 답변:\\n\", res[\"result\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vectordb_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
