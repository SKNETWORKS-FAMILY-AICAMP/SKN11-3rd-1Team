{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b112875",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install typing_extensions==4.7.1 --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d9ab2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install peft datasets transformers bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84924c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install --upgrade pip\n",
    "# 내부적으로 사용하는 툴의 오류를 해결하기 위해"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2fa34f-f6cb-4feb-ab8c-63bec48503ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea08439-e12d-4602-8420-477c57a4f5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bdba0a-20d9-4cef-a23a-e1b213b3d1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64de6025-98b8-4715-8f6a-cfa12c573e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ffb004-69b4-4fae-9336-bc1198654095",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade transformers peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa22dac-29e9-41c8-8cee-fa9949e897cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade torch transformers peft\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ba0b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall numpy -y\n",
    "!pip install \"numpy<2\"\n",
    "!pip install faiss-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c43579-97a7-43dd-91a7-758927574359",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b85c512f-f4dc-4179-991d-676030cbfe67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 성능 체크"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efea2c5-e53e-4364-b57d-90f2629f6136",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import transformers\n",
    "from datasets import load_from_disk\n",
    "from transformers import (\n",
    "    BitsAndBytesConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TextStreamer,\n",
    "    pipeline,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    TrainingArguments\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    prepare_model_for_kbit_training,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    set_peft_model_state_dict,\n",
    "    TaskType,\n",
    "    PeftModel\n",
    ")\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a5b53d-1ada-4683-9b11-9722dfc2d6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODEL = \"yanolja/EEVE-Korean-10.8B-v1.0\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(BASE_MODEL, load_in_4bit=True, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e110e07a-4891-4e80-a150-1ef94c0e5764",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"자율주행자동차에 대해 알려줘\"\n",
    "\n",
    "# 텍스트 생성을 위한 파이프라인 설정\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=256) # max_new_tokens: 생성할 최대 토큰 수\n",
    "outputs = pipe(\n",
    "    prompt,\n",
    "    do_sample=True, # 샘플링 전략 사용. 확률 분포를 기반으로 다음 토큰을 선택\n",
    "    temperature=0.2, # 샘플링의 다양성을 조절하는 파라미터. 값이 높을수록 랜덤성 증가\n",
    "    top_k=50, # 다음 토큰을 선택할 때 상위 k개의 후보 토큰 중에서 선택. 여기에서는 상위 50개의 후보 토큰 중에서 샘플링\n",
    "    top_p=0.95, # 누적 확률이 p가 될 때까지 후보 토큰을 포함\n",
    "    repetition_penalty=1.2, # 반복 패널티를 적용하여 같은 단어나 구절이 반복되는 것 방지\n",
    "    add_special_tokens=True # 모델이 입력 프롬프트의 시작과 끝을 명확히 인식할 수 있도록 특별 토큰 추가\n",
    ")\n",
    "print(outputs[0][\"generated_text\"][len(prompt):]) # 입력 프롬프트 이후에 생성된 텍스트만 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4308645-6663-4593-bd14-a1cd27cf7017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# qlora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb689f0f-8921-4a7f-99a3-b5124c11c12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import transformers\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from langchain.schema import Document\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "418296fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
    "# AutroModelForCausalLM: GPT 기반의 생성형 모델 로드하는 클래스 / DataCollatorForSeq2Seq : 데이터 배치, 데이터 로더와 함꼐 사용, 함수 제공공\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "#PeftModel: 모델 로드드\n",
    "import bitsandbytes as bnb\n",
    "# bnb: 양자화에 도움을 줌/8bit의 연산을 지원해주는 라이브러리/모델을 좀 더 최적화하는데 도움\n",
    "import torch.nn.functional as F\n",
    "# 신경망에 적용하는 함수들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77eb92c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c47b027a",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = {\n",
    "    \"term\": \"./term.json\"\n",
    "}\n",
    "def load_json(path):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "        \n",
    "def convert_list_to_documents(data_list, doc_type):\n",
    "    return [\n",
    "        Document(page_content=json.dumps(item, ensure_ascii=False), metadata={\"type\": doc_type})\n",
    "        for item in data_list\n",
    "    ]\n",
    "dataset_origin= convert_list_to_documents(load_json(file_paths[\"term\"]), \"term\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "38f56716-4e64-47ba-9306-7aeac81fec9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypeIs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1fc6fa",
   "metadata": {},
   "source": [
    "##### 모델 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "794f8e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"yanolja/EEVE-Korean-10.8B-v1.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fd93d078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4-bit 양자화된 모델 로드를 위한 설정\n",
    "bnb_config={\n",
    "    'load_in_4bit':True,                        # 4비트 양자화 적용할 것인지\n",
    "    'bnb_4bit_compute_dtype':torch.float16,     # 4비트의 연산을 수행할 때 어떤 데이터 타입을 쓸 것인지-torch.float16: 속도 최적화\n",
    "    'bnb_4bit_quant_type':'nf4',                # 양자화 방식에 대한 type: nf4: 4-bit의 normal float(교안 참고): 성능 개선을 위해: 정규화화\n",
    "    'device_map':'auto'                         # GPU가 여러 대일때 모두 사용할 수 있게끔 자동 설정\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7a61d9fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90bc64f148fa4b47801c2bde8e98bebe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 토크나이저 및 모델 로드 (모델 로드 시 4-bit 양자화 설정)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name, **bnb_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9ee76e36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaeb31e520b94094ba535b9c98dd74e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"json\", data_files=\"./term.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1a19834b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "da91454f-f189-4bff-88ac-b0aa88370dea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['term', 'desc'],\n",
       "        num_rows: 44\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "affc9818",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data_conversation(examples):\n",
    "    input_texts = []\n",
    "    target_texts = []\n",
    "    \n",
    "    for desc, term in zip(examples['desc'], examples['term']):\n",
    "        # 대화형 프롬프트\n",
    "        input_text = f\"질문: {desc}에 해당하는 용어는?\\n답변:\"\n",
    "        target_text = f\" {term}\"\n",
    "        \n",
    "        input_texts.append(input_text)\n",
    "        target_texts.append(target_text)\n",
    "    \n",
    "    # 입력+출력을 하나로 결합 (GPT 스타일)\n",
    "    full_texts = [inp + tgt for inp, tgt in zip(input_texts, target_texts)]\n",
    "    \n",
    "    tokenized = tokenizer(\n",
    "        full_texts,\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        return_tensors=None\n",
    "    )\n",
    "    \n",
    "    # 자기지도학습용: input_ids를 labels로도 사용\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    \n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb198aa1",
   "metadata": {},
   "source": [
    "##### 파인 튜닝을 위한 LoRA 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2dfc694e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파인튜닝을 위한 LoRA 설정\n",
    "lora_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    lora_dropout=0.1,\n",
    "    bias='none',# 가중치 행렬만, 편향은 없음\n",
    "    task_type='CAUSAL_LM'# lora가 적용될 대상 모델 타입과 맞추어준다.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "75c32e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 20,447,232 || all params: 10,825,371,648 || trainable%: 0.1889\n"
     ]
    }
   ],
   "source": [
    "#LoRA 적용\n",
    "\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "model.enable_input_require_grads()          # 모델에 대해 입력값으로 받은 값에 대해 gradient를 사용하게끔하는 설정\n",
    "model.gradient_checkpointing_enable()       # 체크포인트로 중간 저장: 모델이 커지게 되면 중간에 수행해준 내용들을 저장하는것이 효율적\n",
    "model.print_trainable_parameters()          # 학습가능한 파라미터의 수를 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd1c446",
   "metadata": {},
   "source": [
    "##### Training 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "051d8f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 설정\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)  # 동적 데이터를 저장하면서 자동으로 다음 데이터를 불러옴: model의 타입에 맞추어 tokenizer을 사용함/\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = './q_lora_korqa',      # \n",
    "    eval_strategy='no',              # 검증: 하지 않음\n",
    "    save_strategy='steps',              #\n",
    "    per_device_train_batch_size=4,      # 배치 사이즈: 트레인\n",
    "    per_device_eval_batch_size=4,       # 배치 사이즈: eval\n",
    "    gradient_accumulation_steps=8,      # 가중치를 누산하여 한번에 계산 : 8번만큼의 가중치를 누적해 놓았다가 한번에 계산\n",
    "    learning_rate=2e-4,                 # 학습률\n",
    "    weight_decay=0.01,                  # L2 정규화 적용 비율\n",
    "    num_train_epochs=10,                 # 몇번의 epoch을 진행할 것인지\n",
    "    logging_dir='./logs',               # log 남길 dir\n",
    "    logging_steps=100,                  # log 남길 빈도수\n",
    "    save_total_limit=2,                 # 체크포인트 최대 개수: 가장 최근의 2개 저장\n",
    "    fp16=True,                          # \n",
    "    push_to_hub=False,                  # hub: hugginface에서의 허브 - true: 자동으로 hub에 저장\n",
    "    report_to='none',                    # 학습결과를 표현할 수 있는 툴에 전달달\n",
    "    remove_unused_columns=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f53ddf71-5819-47d3-ad24-9c493109426b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9ada82b5b0648899b61e892b12ad183",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/44 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1094/1096101301.py:4: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = dataset['train'].map(preprocess_data_conversation, batched=True, remove_columns=dataset['train'].column_names)\n",
    "\n",
    "# Trainer 설정\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,  # 전처리된 데이터셋 사용\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "aa78f74b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20/20 03:21, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=20, training_loss=0.1571187734603882, metrics={'train_runtime': 216.9381, 'train_samples_per_second': 2.028, 'train_steps_per_second': 0.092, 'total_flos': 1.330835763265536e+16, 'train_loss': 0.1571187734603882, 'epoch': 10.0})"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 학습\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ce19b9f9-8c5b-4623-a12a-a9a5a9bccde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장\n",
    "trainer.save_model(\"workspace/q_lora_korqa/checkpoint-15\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a16b4a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "trained_model_path='./q_lora_korqa/checkpoint-15'\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_name) # 원본 모델에 대한 설정 로드\n",
    "config.save_pretrained(trained_model_path)      # 체크포인트 경로에 설정 저장: 재사용하기 위해해"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a610d292",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f772d5037b854969b65450414969ab6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Can't find 'adapter_config.json' at './q_lora_korqa/checkpoint-15'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/config.py:260\u001b[0m, in \u001b[0;36mPeftConfigMixin._get_peft_type\u001b[0;34m(cls, model_id, **hf_hub_download_kwargs)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 260\u001b[0m     config_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mCONFIG_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhf_hub_download_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py:106\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 106\u001b[0m     \u001b[43mvalidate_repo_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m arg_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py:154\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m repo_id\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must be in the form \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrepo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnamespace/repo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Use `repo_type` argument if needed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m     )\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX\u001b[38;5;241m.\u001b[39mmatch(repo_id):\n",
      "\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': './q_lora_korqa/checkpoint-15'. Use `repo_type` argument if needed.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[82], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m adapter_model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./q_lora_korqa/checkpoint-15\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      4\u001b[0m base_model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name, torch_dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m, device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mPeftModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_model_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/peft_model.py:439\u001b[0m, in \u001b[0;36mPeftModel.from_pretrained\u001b[0;34m(cls, model, model_id, adapter_name, is_trainable, config, autocast_adapter_dtype, ephemeral_gpu_offload, low_cpu_mem_usage, **kwargs)\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;66;03m# load the config\u001b[39;00m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    438\u001b[0m     config \u001b[38;5;241m=\u001b[39m PEFT_TYPE_TO_CONFIG_MAPPING[\n\u001b[0;32m--> 439\u001b[0m         \u001b[43mPeftConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_peft_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[43m            \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msubfolder\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrevision\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcache_dir\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m            \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muse_auth_token\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtoken\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    447\u001b[0m     ]\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PeftConfig):\n\u001b[1;32m    449\u001b[0m     config\u001b[38;5;241m.\u001b[39minference_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m is_trainable\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/config.py:266\u001b[0m, in \u001b[0;36mPeftConfigMixin._get_peft_type\u001b[0;34m(cls, model_id, **hf_hub_download_kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m         config_file \u001b[38;5;241m=\u001b[39m hf_hub_download(\n\u001b[1;32m    261\u001b[0m             model_id,\n\u001b[1;32m    262\u001b[0m             CONFIG_NAME,\n\u001b[1;32m    263\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhf_hub_download_kwargs,\n\u001b[1;32m    264\u001b[0m         )\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m--> 266\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCONFIG_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m at \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    268\u001b[0m loaded_attributes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_json_file(config_file)\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loaded_attributes[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpeft_type\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: Can't find 'adapter_config.json' at './q_lora_korqa/checkpoint-15'"
     ]
    }
   ],
   "source": [
    "# 학습시킨 adaptor 이어붙이기\n",
    "adapter_model_path = './q_lora_korqa/checkpoint-15'\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype='auto', device_map='auto')\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model, adapter_model_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174cd704-ba79-41ab-9d55-d1dfaefbbc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY= "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d58aef",
   "metadata": {},
   "source": [
    "##### API 제한 -> 순서대로 넣기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "749b3dce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1094/153068654.py:34: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectorstore.persist()\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma  # persist 지원\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 1. 청크 크기 조정 (500~1000 권장)\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "# 2. 문서 분할\n",
    "all_splits = text_splitter.split_documents(dataset_origin)\n",
    "\n",
    "embedding_model = OpenAIEmbeddings(model='text-embedding-3-large', openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "# 4. Chroma DB에 배치 처리로 저장\n",
    "batch_size = 100  # 한 번에 처리할 청크 수\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=all_splits[:batch_size],  # 첫 배치\n",
    "    embedding=embedding_model,\n",
    "    persist_directory=\"./vectordb1\"\n",
    ")\n",
    "\n",
    "# 남은 청크를 순차적으로 추가\n",
    "for i in range(batch_size, len(all_splits), batch_size):\n",
    "    batch = all_splits[i:i+batch_size]\n",
    "    vectorstore.add_documents(\n",
    "        documents=batch,\n",
    "        embedding=embedding_model\n",
    "    )\n",
    "\n",
    "vectorstore.persist()  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "61c3c7a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI\n",
    "from transformers import pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "\n",
    "# 2. 프롬프트 템플릿\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"<|instruction|>\n",
    "주어진 문서에서 질문과 가장 관련성이 높은 정보를 우선적으로 활용하여 답변하세요.\n",
    "문서에 없는 내용은 추측하지 말고, 문서 기반으로만 답변하세요.\n",
    "\n",
    "<|query|>\n",
    "사용자 질문: {question}\n",
    "\n",
    "<|retrieved_context|>\n",
    "{context}\n",
    "\n",
    "<|response_format|>\n",
    "1. 직접 답변 (한 문장)\n",
    "2. 상세 설명 (2-3문장)  \n",
    "3. 법적 근거 (있는 경우)\n",
    "4. 추가 참고사항\n",
    "\n",
    "<|system|>\n",
    "다음 예시들을 참고하여 동일한 형식과 품질로 답변하세요.\n",
    "\n",
    "<|example1|>\n",
    "질문: 자율주행시스템이란?\n",
    "답변: \"「자율주행자동차 상용화 촉진 및 지원에 관한 법률」 제2조 제1항 제2호에 따른 자율주행시스템을 말한다. 이 경우 그 종류는 완전 자율주행시스템, 부분 자율주행시스템 등 행정안전부령으로 정하는 바에 따라 세분할 수 있다.\"\n",
    "\n",
    "<|example2|>  \n",
    "질문: 보행자우선도로이란?\n",
    "답변: 보행안전 및 편의증진에 관한 법률」 제2조제3호에 따른 보행자우선도로를 말한다.\n",
    "\n",
    "<|user|>\n",
    "질문: {question}\n",
    "참고문서: {context}\n",
    "\n",
    "<|assistant|>\n",
    "**{question}**\n",
    "\n",
    "<|answer|>\n",
    "**직접답변**: {question}는 \n",
    "\n",
    "**상세설명**: \n",
    "\n",
    "\n",
    "**법적근거**: \n",
    "\n",
    "\n",
    "**참고사항**: \"\"\",\n",
    "    input_variables=[\"question\", \"context\"]\n",
    ")\n",
    "\n",
    "# Few-shot 학습 강화 프롬프트\n",
    "few_shot_prompt = PromptTemplate(\n",
    "    template=\"\"\"<|system|>\n",
    "다음 예시들을 참고하여 동일한 형식과 품질로 답변하세요.\n",
    "\n",
    "<|example1|>\n",
    "질문: 자율주행시스템이란?\n",
    "답변: \"「자율주행자동차 상용화 촉진 및 지원에 관한 법률」 제2조 제1항 제2호에 따른 자율주행시스템을 말한다. 이 경우 그 종류는 완전 자율주행시스템, 부분 자율주행시스템 등 행정안전부령으로 정하는 바에 따라 세분할 수 있다.\"\n",
    "\n",
    "<|example2|>  \n",
    "질문: 보행자우선도로이란?\n",
    "답변: 보행안전 및 편의증진에 관한 법률」 제2조제3호에 따른 보행자우선도로를 말한다.\n",
    "\n",
    "<|user|>\n",
    "질문: {question}\n",
    "참고문서: {context}\n",
    "\n",
    "<|assistant|>\n",
    "**{question}**\"\"\",\n",
    "    input_variables=[\"question\", \"context\"]\n",
    ")\n",
    "\n",
    "# 3. 리트리버 설정\n",
    "retriever_chroma = vectorstore.as_retriever(\n",
    "    search_kwargs={\n",
    "        \"k\": 5\n",
    "        }\n",
    ")\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "# 1.4 LangChain 호환 래퍼 적용\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "65490bcc-773e-4afa-9076-99935d9009cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "자율주행시스템은 차량 스스로 주행환경을 인식하여 차량의 위치와 방향, 주행 상태를 판단하고, 안전운행을 위한 제동, 조향, 가감속 등의 조치를 수행하여 안전운행을 지원한다.\n",
      "자율주행시스템은 자율주행시스템의 종류에 따라 자율주행지원시스템, 자율주행시스템, 자율주행완성시스템으로 구분한다.\n",
      "자율주행지원시스템은 자율주행시스템의 종류 중 자율주행시스템의 작동이 필요한 상황에서 운전자가 자율주행시스템 작동에 필요한 조작을 한 경우에만 자율주행시스템이 작동하는 시스템을 말한다.\n",
      "자율주행시스템은 자율주행시스템의 종류 중 자율주행시스템의 작동이 필요한 상황에서 자율주행시스템이 작동하는 시스템을 말한다.\n",
      "자율주행완성시스템은 자율주행시스템의 종류 중 자율주행시스템이 작동하는 시스템을 말한다.\n",
      "자율주행시스템은 자율주행시스템의 종류에 따라 자율주행지원시스템, 자율주행시스템, 자율주행완성시스템으로 구분한다.\n",
      "자율주행지원시스템은 자율주행시스템의 종류 중 자율주행시스템의 작동이 필요한 상황에서 운전자가 자율주행시스템 작동에 필요한 조작을 한 경우에만 자율주행시스템이 작동하는 시스템을 말한다.\n",
      "자율주행시스템은 자율주행시스템의 종류 중 자율주행시스템의 작동이 필요한 상황에서 자율주행시스템이 작동하는 시스템을 말한다.\n",
      "자율주행완성시스템은 자율주행시스템의 종류 중 자율주행시스템이 작동하는 시스템을 말한다.\n",
      "자율주행시스템은 자율주행시스템의 종류에 따라 자율주행지원시스템, 자율주행시스템, 자율주행완성시스템으로 구분한다.\n",
      "자율주행지원시스템은 자율주행시스템의 종류 중 자율주행시스템의 작동이 필요한 상황에서 운전자가 자율주행시스템 작동에 필요한 조작을 한\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 필터 적용 후 >> Chroma 답변:\n",
      " <|instruction|>\n",
      "주어진 문서에서 질문과 가장 관련성이 높은 정보를 우선적으로 활용하여 답변하세요.\n",
      "문서에 없는 내용은 추측하지 말고, 문서 기반으로만 답변하세요.\n",
      "\n",
      "<|query|>\n",
      "사용자 질문: 자율주행시스템에 대해 알려줘\n",
      "\n",
      "<|retrieved_context|>\n",
      "{\"term\": \"자율주행시스템\", \"desc\": [\"「자율주행자동차 상용화 촉진 및 지원에 관한 법률」 제2조 제1항 제2호에 따른 자율주행시스템을 말한다.\", \"이 경우 그 종류는 완전 자율주행시스템, 부분 자율주행시스템 등 행정안전부령으로 정하는 바에 따라 세분할 수 있다.\"]}\n",
      "\n",
      "{\"term\": \"자율주행자동차\", \"desc\": [\"「자동차관리법」 제2조 제1호의 3에 따른 자율주행자동차로서 자율주행시스템을 갖추고 있는 자동차를 말한다.\"]}\n",
      "\n",
      "{\"term\": \"운전\", \"desc\": [\"도로(제27조제6항제3호·제44조·제45조·제54조제1항·제148조·제148조의 2 및 제156조 제10호의 경우에는 도로 외의 곳을 포함한다)에서 차마 또는 노면전차를 그 본래의 사용 방법에 따라 사용하는 것(조종 또는 자율주행시스템을 사용하는 것을 포함한다)을 말한다.\"]}\n",
      "\n",
      "{\"term\": \"자동차\", \"desc\": [\"철길이나 가설된 선을 이용하지 아니하고 원동기를 사용하여 운전되는 차(견인되는 자동차도 자동차의 일부로 본다)로서 다음 각 목의 차를 말한다.\", \"「자동차관리법」 제3조에 따른 다음의 자동차(승용자동차, 승합자동차, 화물자동차, 특수자동차, 이륜자동차, 「건설기계관리법」 제26조제1항 단서에 따른 건설기계). \", \"다만, 원동기장치자전거는 제외한다.\"]}\n",
      "\n",
      "{\"term\": \"앞지르기\", \"desc\": [\"차의 운전자가 앞서가는 다른 차의 옆을 지나서 그 차의 앞으로 나가는 것을 말한다.\"]}\n",
      "\n",
      "<|response_format|>\n",
      "1. 직접 답변 (한 문장)\n",
      "2. 상세 설명 (2-3문장)  \n",
      "3. 법적 근거 (있는 경우)\n",
      "4. 추가 참고사항\n",
      "\n",
      "<|system|>\n",
      "다음 예시들을 참고하여 동일한 형식과 품질로 답변하세요.\n",
      "\n",
      "<|example1|>\n",
      "질문: 자율주행시스템이란?\n",
      "답변: \"「자율주행자동차 상용화 촉진 및 지원에 관한 법률」 제2조 제1항 제2호에 따른 자율주행시스템을 말한다. 이 경우 그 종류는 완전 자율주행시스템, 부분 자율주행시스템 등 행정안전부령으로 정하는 바에 따라 세분할 수 있다.\"\n",
      "\n",
      "<|example2|>  \n",
      "질문: 보행자우선도로이란?\n",
      "답변: 보행안전 및 편의증진에 관한 법률」 제2조제3호에 따른 보행자우선도로를 말한다.\n",
      "\n",
      "<|user|>\n",
      "질문: 자율주행시스템에 대해 알려줘\n",
      "참고문서: {\"term\": \"자율주행시스템\", \"desc\": [\"「자율주행자동차 상용화 촉진 및 지원에 관한 법률」 제2조 제1항 제2호에 따른 자율주행시스템을 말한다.\", \"이 경우 그 종류는 완전 자율주행시스템, 부분 자율주행시스템 등 행정안전부령으로 정하는 바에 따라 세분할 수 있다.\"]}\n",
      "\n",
      "{\"term\": \"자율주행자동차\", \"desc\": [\"「자동차관리법」 제2조 제1호의 3에 따른 자율주행자동차로서 자율주행시스템을 갖추고 있는 자동차를 말한다.\"]}\n",
      "\n",
      "{\"term\": \"운전\", \"desc\": [\"도로(제27조제6항제3호·제44조·제45조·제54조제1항·제148조·제148조의 2 및 제156조 제10호의 경우에는 도로 외의 곳을 포함한다)에서 차마 또는 노면전차를 그 본래의 사용 방법에 따라 사용하는 것(조종 또는 자율주행시스템을 사용하는 것을 포함한다)을 말한다.\"]}\n",
      "\n",
      "{\"term\": \"자동차\", \"desc\": [\"철길이나 가설된 선을 이용하지 아니하고 원동기를 사용하여 운전되는 차(견인되는 자동차도 자동차의 일부로 본다)로서 다음 각 목의 차를 말한다.\", \"「자동차관리법」 제3조에 따른 다음의 자동차(승용자동차, 승합자동차, 화물자동차, 특수자동차, 이륜자동차, 「건설기계관리법」 제26조제1항 단서에 따른 건설기계). \", \"다만, 원동기장치자전거는 제외한다.\"]}\n",
      "\n",
      "{\"term\": \"앞지르기\", \"desc\": [\"차의 운전자가 앞서가는 다른 차의 옆을 지나서 그 차의 앞으로 나가는 것을 말한다.\"]}\n",
      "\n",
      "<|assistant|>\n",
      "**자율주행시스템에 대해 알려줘**\n",
      "\n",
      "<|answer|>\n",
      "**직접답변**: 자율주행시스템에 대해 알려줘는 \n",
      "\n",
      "**상세설명**: \n",
      "\n",
      "\n",
      "**법적근거**: \n",
      "\n",
      "\n",
      "**참고사항**: \n",
      "\n",
      "\n",
      "<|AI|>\n",
      "자율주행시스템은 자율주행자동차 상용화 촉진 및 지원에 관한 법률 제2조 제1항 제2호에 따른 자율주행시스템을 말한다. 이 경우 그 종류는 완전 자율주행시스템, 부분 자율주행시스템 등 행정안전부령으로 정하는 바에 따라 세분할 수 있다.\n",
      "\n",
      "자율주행자동차는 자동차관리법 제2조 제1호의 3에 따른 자율주행자동차로서 자율주행시스템을 갖추고 있는 자동차를 말한다.\n",
      "\n",
      "운전은 도로(제27조제6항제3호·제44조·제45조·제54조제1항·제148조·제148조의 2 및 제156조 제10호의 경우에는 도로 외의 곳을 포함한다)에서 차마 또는 노면전차를 그 본래의 사용 방법에 따라 사용하는 것(조종 또는 자율주행시스템을 사용하는 것을 포함한다)을 말한다.\n",
      "\n",
      "자동차는 철길이나 가설된 선을 이용하지 아니하고 원동기를 사용하여 운전되는 차(견인되는 자동차도 자동차의 일부로 본다)로서 다음 각 목의 차를 말한다.\n",
      "\n",
      "자율주행시스템은 완전 자율주행시스템, 부분 자율주행시스템 등으로 분류되며, 운전자가 차량을 제어하지 않아도 차량 스스로 안전한 주행을 수행할 수 있다.\n",
      "\n",
      "운전은 운전자가 차량을 제어하여 주행하는 것을 의미하며, 자율주행시스템을 사용할 경우에도 운전자가 차량을 제어해야 한다.\n",
      "\n",
      "자동차는 철길이나 가설된 선을 이용하지 않고 원동기를 사용하여 운전되는 차로, 승용자동차, 승합자동차, 화물자동차, 특수자동차, 이륜자동차, 건설기계 등이 있다.\n",
      "\n",
      "운전자는 앞지르기를 할 때 앞차의 옆을 지나서 앞차의 앞으로 나가는 것을 의미한다.\n"
     ]
    }
   ],
   "source": [
    "# 4. QA 체인 구성\n",
    "qa_chain_chroma = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever_chroma,\n",
    "    chain_type=\"stuff\",\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")\n",
    "\n",
    "# 3. 직접 generate() 호출\n",
    "def generate_text(querry, max_length=512):\n",
    "    inputs = tokenizer(querry, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=max_length,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9\n",
    "    )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# 실행 예시\n",
    "print(generate_text(\"자율주행시스템\"))\n",
    "# # 5. 실제 질의 실행\n",
    "query = \"자율주행시스템에 대해 알려줘\"\n",
    "res_chroma = qa_chain_chroma.invoke({\"query\": query})\n",
    "\n",
    "# # 6. 출력\n",
    "print(\"✅ 필터 적용 후 >> Chroma 답변:\\n\", res_chroma[\"result\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fad448",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
